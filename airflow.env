# airflow.env
# Todas as variáveis de ambiente do Airflow.
# Carregado via env_file no docker-compose.yml — sem parsing YAML, sem conflitos.
#
# Preencha os valores marcados com TODO antes de subir a stack.

# ── Segurança (OBRIGATÓRIO preencher) ─────────────────────────────────────────
# python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"

# ── Configurações do Airflow ────────────────────────────────────────────────
# airflow.env
# Todas as variáveis de ambiente do Airflow.
# Carregado via env_file no docker-compose.yml — sem parsing YAML, sem conflitos.
#
# Preencha os valores marcados com TODO antes de subir a stack.

# ── Segurança (OBRIGATÓRIO preencher) ─────────────────────────────────────────
# python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW__CORE__FERNET_KEY= FEibn8ApcyenqJL8x7Oy0IakKkOmY4uFGoDvlt9kAQ8=


# python -c "import secrets; print(secrets.token_hex(32))"

AIRFLOW__WEBSERVER__SECRET_KEY= a2f278cc9e416488ca7a3499b4178a3acaa6cd498e6d7a94f9fc58eef2c8c705

# ── Core ──────────────────────────────────────────────────────────────────────
AIRFLOW__CORE__EXECUTOR=CeleryExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=false
AIRFLOW__CORE__DEFAULT_TIMEZONE=America/Sao_Paulo
AIRFLOW__CORE__PARALLELISM=32
AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=16
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=2

# ── Banco de metadados ────────────────────────────────────────────────────────
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow

# ── Celery ────────────────────────────────────────────────────────────────────
AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0
AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres:5432/airflow
AIRFLOW__CELERY__WORKER_CONCURRENCY=8

# ── Webserver ─────────────────────────────────────────────────────────────────
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=false

# ── Scheduler ─────────────────────────────────────────────────────────────────
AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK=true
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=30
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30

# ── API ───────────────────────────────────────────────────────────────────────
AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session

# ── Variáveis do projeto (lidas pelas DAGs via Variable.get) ──────────────────
AIRFLOW_VAR_SPARK_MASTER=spark://spark-master:7077
AIRFLOW_VAR_EVENTS_BUCKET=s3a://microservices-events
AIRFLOW_VAR_OUTPUT_BUCKET=s3a://microservices-output

# ── Java / PySpark ────────────────────────────────────────────────────────────
JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
SPARK_HOME=/opt/spark
PYSPARK_PYTHON=python3.11
PYSPARK_DRIVER_PYTHON=python3.11

# ── AWS ───────────────────────────────────────────────────────────────────────
AWS_DEFAULT_REGION=us-east-1
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

# ── PYTHONPATH — garante que dags/ e plugins/ ficam no sys.path ───────────────
PYTHONPATH=/opt/airflow/dags:/opt/airflow/plugins:/opt/airflow

# ── Corrige versao Python (imagem 2.8.1 usa Python 3.8, nao 3.11) ─────────────
PYSPARK_PYTHON=python3.8
PYSPARK_DRIVER_PYTHON=python3.8